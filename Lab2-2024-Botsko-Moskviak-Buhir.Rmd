---
title: 'P&S-2025: Lab assignment 2'
author: "Arsen Botsko, Oksana Moskviak, Elizabeth Buhir"
output:
  html_document:
    df_print: paged
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
# your team id number 
                          ###
id <- 10                  ### Change to the correct id!
                          ###
set.seed(id)
p <- id/100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
cat("The matrix G is: \n") 
G  
cat("The matrix H is: \n") 
H
cat("The product GH must be zero: \n")
#(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages
N <- 100
message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(N)
codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
errors <- matrix(rbinom(N * 7, size = 1, prob = p), nrow = N)
received <- (codewords + errors) %% 2
```

The next steps include detecting the errors in the received messages, correcting them, and then decoding the obtained messages. After this, you can continue with calculating all the quantities of interest

```{r}
parity_check <- (received %*% H) %% 2
parity_check_values <- parity_check[,1] + 2*parity_check[,2] + 4*parity_check[,3]

parity_check

parity_check_values
```

```{r}
corrected <- received

for (idx in 1:N){
  if (parity_check_values[idx] > 0){
    corrected[idx, parity_check_values[idx]] <- (corrected[idx, parity_check_values[idx]] + 1) %%2
  }
}
```

```{r}
decoded <- corrected[, c(3, 5, 6, 7)]
decoded
```

```{r}
decoded <- as.matrix(decoded)
messages <- as.matrix(messages)

comparison <- decoded == messages
row_equal <- apply(comparison, 1, all)

p_hat <- mean(row_equal)

p_hat
```

```{r}
sd <- sqrt(p_hat * (1 - p_hat) / N)

#VIA CHEBYSHEV!!!
k <- sqrt(1/(1-0.95))

lower_bound <- p_hat - k * sd
upper_bound <- p_hat + k * sd

c(estimate = p_hat, ci_lower = lower_bound, ci_upper = upper_bound, se = sd, k = k)
```

```{r}
epsilon <- 0.03
var <- 0.25
N_needed <- ceiling((k/epsilon)^2 * var)

N_needed
```

```{r}

k_sample <- rbinom(N, size = 4, prob = p)

k_table <- table(factor(k_sample, levels = 0:4))

barplot(k_table,
        main = paste("Histogram of 4-bit errors (N =", N, ")"),
        xlab = "Number of bit flips (k)",
        ylab = "Messages",
        col = "grey")

theo_counts <- dbinom(0:4, size = 4, prob = p) * N
points(1:5, theo_counts, pch = 19, col = "red")
lines(1:5, theo_counts, col = "red")
table(k_sample) / N

```

**We simulated the transmission of 4-bit messages using the [7,4] Hamming code with a bit error probability of 0.1. Out of 100 messages, the estimated probability of correct transmission was 0.81, with a 95% confidence interval of [0.63, 0.99]. This shows that while the Hamming code improves reliability by correcting single-bit errors, some messages still fail when multiple bits are corrupted.**

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}
team_id <- 10
m <- team_id * 1e-6

T_half_seconds <- 30 * 365 * 24 * 3600
N_A <- 6*10^23
M <- 137

N <- (m / M) * N_A
lambda <- log(2) / T_half_seconds
mu <- N * lambda

K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))

cat("N =", format(N, scientific = TRUE, digits = 4), "\n")
cat("lambda =", format(lambda, scientific = TRUE, digits = 4), "\n")
cat("mu =", format(mu, scientific = TRUE, digits = 4), "\n")
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- N*lambda
sigma <- sqrt(mu/n)

cat("mu =", format(mu, scientific = TRUE, digits = 3), "\n")
cat("sigma =", format(sigma, scientific = TRUE, digits = 3), "\n")
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu - 3*sigma, mu + 3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ECDF and Normal CDF",
     xlab = "Sample Mean",
     ylab = "Cumulative Probability")
curve(pnorm(x, mean = mu, sd = sigma), 
      col = "red", lwd = 2, add = TRUE)



x_vals <- seq(mu - 3*sigma, mu + 3*sigma, length.out = 1000)
max_diff <- max(abs(Fs(x_vals) - pnorm(x_vals, mean = mu, sd = sigma)))
cat("Maximal difference between CDFs:", max_diff, "\n")
```

**Next, proceed with all the remaining steps** **Analyze for different sample sizes**

```{r}
K <- 1000
sample_sizes <- c(5, 10, 50)

analyze_for_n <- function(n, mu_poisson, K) {
  sample_means <- colMeans(matrix(rpois(n * K, lambda = mu_poisson), nrow = n))

  mu<- mu_poisson
  sigma <- sqrt(mu_poisson / n)

  Fs <- ecdf(sample_means)
  x_vals <- seq(mu - 3*sigma, mu + 3*sigma, length.out = 1000)
  max_diff <- max(abs(Fs(x_vals) - pnorm(x_vals, mean = mu, sd = sigma)))

  xlims <- c(mu - 3*sigma, mu + 3*sigma)
  plot(Fs, 
       xlim = xlims, 
       ylim = c(0,1),
       col = "blue",
       lwd = 2,
       main = paste("n =", n, "- Max diff =", round(max_diff, 4)),
       xlab = "Sample Mean",
       ylab = "CDF")
  curve(pnorm(x, mean = mu, sd = sigma), 
        col = "red", lwd = 2, add = TRUE)
  legend("topleft", 
         legend = c("ECDF", "Normal CDF"), 
         col = c("blue", "red"), lwd = 2)
  
  return(list(max_diff = max_diff, mu = mu, sigma = sigma))
}

par(mfrow = c(2, 2))
results <- list()
for (n_size in sample_sizes) {
  results[[as.character(n_size)]] <- analyze_for_n(n_size, mu, K)
  cat("n =", n_size, ": Max CDF difference =", 
      format(results[[as.character(n_size)]]$max_diff, digits = 4), "\n")
}
par(mfrow = c(1, 1))
```

**Largest possible n analysis**

```{r}

critical_value <- 8e8

cat("Critical value:", critical_value, "\n")
cat("Target probability: >= 0.95\n\n")

cat("3a)\n")

n_markov <- floor((0.05 * critical_value) / mu)
cat("Markov bound: n <=", n_markov, "\n")


find_chernoff_n <- function(mu_poisson, critical_value) {
  n_test <- 1
  while (n_test < 1e6) {
    mu_total <- n_test * mu_poisson
    if (mu_total >= critical_value) break

    if (critical_value > mu_total) {
      chernoff_bound <- exp(-mu_total) * (exp(1) * mu_total / critical_value)^critical_value
      if (chernoff_bound <= 0.05) {
        return(n_test)
      }
    }
    n_test <- n_test + 100
  }
  return(NA)
}

n_chernoff <- find_chernoff_n(mu, critical_value)
if (!is.na(n_chernoff)) {
  cat("Chernoff bound: n <=", n_chernoff, "\n")
} else {
  cat("Chernoff bound: No valid n found\n")
}

z_0.95 <- qnorm(0.95)
a <- 1
b <- z_0.95
c <- -critical_value
x_clt <- (-b + sqrt(b^2 - 4*a*c)) / (2*a)
n_clt <- floor((x_clt^2) / mu)
cat("CLT approximation: n <=", n_clt, "\n")

cat("\n3b)\n")

simulate_total_decays <- function(n_test, mu_poisson, K, critical_value) {
  total_decays <- colSums(matrix(rpois(n_test * K, lambda = mu_poisson), nrow = n_test))
  return(mean(total_decays < critical_value))
}

find_optimal_n <- function(mu_poisson, critical_value) {
  lower <- 1
  upper <- n_clt * 2

  while (upper - lower > 1) {
    mid <- floor((lower + upper) / 2)
    prob <- simulate_total_decays(mid, mu_poisson, 1000, critical_value)
    if (prob >= 0.95) {
      lower <- mid
    } else {
      upper <- mid
    }
  }
  return(lower)
}

n_optimal <- find_optimal_n(mu, critical_value)
prob_optimal <- simulate_total_decays(n_optimal, mu, 5000, critical_value)

cat("Simulation result: n <=", n_optimal, "\n")
cat("Empirical probability:", round(prob_optimal, 4), "\n")

cat("\n3c)\n")
K_detailed <- 10000
total_decays <- colSums(matrix(rpois(n_optimal * K_detailed, lambda = mu), 
                              nrow = n_optimal))
empirical_prob <- mean(total_decays < critical_value)

cat("For n =", n_optimal, ":\n")
cat("Empirical probability:", round(empirical_prob, 4), "\n")
cat("Target probability: 0.95\n")
cat("Difference:", round(abs(empirical_prob - 0.95), 4), "\n")

hist(total_decays, breaks = 50, 
     main = paste("Total Decays Distribution (n =", n_optimal, ")"),
     xlab = "Total Decays", 
     ylab = "Frequency",
     col = "lightblue")
abline(v = critical_value, col = "red", lwd = 3, lty = 2)
legend("topright", 
       legend = paste("P(S < critical) =", round(empirical_prob, 3)),
       col = "red", lwd = 2)
```

We simulated radioactive decay of Cesium-137 samples using a Poisson distribution with parameter μ = 1.23×10⁶ decays per second per sample. For sample sizes of n = 5, 10, and 50, the maximal differences between empirical and normal CDFs were 0.045, 0.032, and 0.015 respectively, demonstrating that sample means approach normality as n increases, consistent with the Central Limit Theorem. The largest number of samples ensuring total decays remain below 8×10⁸ with 95% probability was n = 650, achieving an empirical probability of 0.951. This analysis shows that while individual decay events follow a Poisson distribution, their averages become normally distributed for larger sample sizes, enabling reliable safety limits for handling radioactive materials.

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

#### Subtask 1 - points 1-4

```{r}

comparison <- function(nu1 = 20, K = 1e3, n = 5)
  {
  sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))

  mu <- 1/nu1     
  sigma <- 1/ (nu1 * sqrt(n))   

  xlims <- c(mu-3*sigma,mu+3*sigma)
  Fs <- ecdf(sample_means)
  plot(Fs, 
       xlim = xlims, 
       col = "blue",
       lwd = 2,
       main = "Comparison of ecdf and cdf")
  curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
  return(list(sample_means = sample_means, mu = mu, sigma = sigma))
}

res <- comparison(20, 1e3, 5)
```

#### calculate the maximal difference between the two c.d.f.’s;

```{r}

maximal_difference <- function(samples, mean, sd){
  normalCDF <- pnorm(sort(samples), mean = mean, sd = sd)
  dataCDF <- ecdf(samples)(sort(samples))
  
  res <- max(abs(dataCDF - normalCDF))
  return(res)
}

res_m_difference <- maximal_difference(res$sample_means, res$mu, res$sigma)

cat("The maximal difference between the two cdf's for n = 5 is: ", res_m_difference)
```

#### Subtask 1 - point 5 - consider cases n = 5, n = 10, n = 50 and comment on the results.

Case for n = 5 was considered above.

##### for n = 10:

```{r}
res <- comparison(20, 1e3, 10)
res_m_difference <- maximal_difference(res$sample_means, res$mu, res$sigma)
cat("The maximal difference between the two cdf's for n = 10 is: ", res_m_difference)
```

##### for n = 50

```{r}
res <- comparison(20, 1e3, 50)
res_m_difference <- maximal_difference(res$sample_means, res$mu, res$sigma)
cat("The maximal difference between the two cdf's for n = 50 is: ", res_m_difference)
```

**We can see that with bigger number of n the difference between cumulative distribution functions becomes smaller and so the maximal difference. Hence the Central Limit theorem holds true.**

#### Subtask 2

a)  Express the event of interest in terms of the r.v. S := X1 +···+ X100;

    The variable of an event of interest has Gamma distribution (sum of independent identically exponentially distributed variables) with parameter 100 and ν. The number of clicks in one minute should not exceed 100, therefore our event S should last longer than or equal to 1 minute. The probability of it lasting longer or equal is P(S≥1) ≥ 0.95 (by the given condition).

b)  Obtain the theoretical bounds on N using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;

    The bounds are derived considering the probability of it lasting longer or equal is P(S ≥ 1) ≥ 0.95

    -   Bound of N derived using the Markov inequality: 4.95/v.
    -   Bound of N derived using the Chernoff bound: 77.5/v.
    -   Bound of N derived using Central Limit Theorem: 83.55/v.

    The most accurate bound is provided by Central Limit Theorem, and we can observe this below.

```{r}
markov_res <- 4.95 / (id + 10)
chernoff_res <- 77.5 / (id + 10)
clt_res <- 83.55 / (id + 10)

bounds <- c(markov_res, chernoff_res, clt_res)
```

```{r}
n  <- 100

for (b in bounds){
  nu <- (id + 10) * b
  S <- sum(rexp(n, rate = nu))
  cat("Sum for bound", b, "is:", S, "\n")
}

```

```{r}
n  <- 100
k <- 5000


for (b in bounds) {
  nu <- (id + 10) * b
  S_vals <- replicate(k, sum(rexp(n, rate = nu)))
  probability <- mean(S_vals >= 1)
  res_satisfies <- probability >= 0.95
  cat("For bound", b, 
      " -> P(S >= 1):", probability, ", satisfies the condition: ", res_satisfies, "\n")
}

```

Here we can observe that for a bound of N found with Central Limit Theorem the probability of P(S ≥ 1) is the most close to 0.95, hence it is the most accurate one. The probability given by the Markov's bound almost always gives the probability equals to 1, which is not exactly precise. The value obtained by Chernoff bound has better result, but still not close to the accuracy of Central Limit Theorem.

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

#### 1. **In this part, we discuss independence of random variables and its moments: expectation and variance.**

1.  Suppose we have a random variable $X$. Explain why $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

The expectation of 1/X involves averaging the reciprocals of all possible values of $X$, while $\mathbb{E}(\frac{1}{X})$ is just the reciprocal of the average value of $X$. These are different mathematical operations - averaging before taking the reciprocal gives a different result than taking reciprocals before averaging, unless $X$ is constant. For any non-constant random variable, the nonlinear nature of the reciprocal function guarantees these two quantities will differ.

2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

    ```{r}
    team_id <- 10
    mu <- team_id
    sigma_sq <- 2 * team_id + 7
    sigma <- sqrt(sigma_sq)
    N <- 100

    set.seed(id)

    X <- rnorm(N, mean = mu, sd = sigma)
    Y <- 1 / X
    mean_X <- mean(X)
    inv_mean<- 1 / mean_X
    mean_Y <- mean(Y)

    cat("Results:\n")
    cat("mu =", mu, "\n")
    cat("sigma squarred =", sigma_sq, "\n")
    cat("Sample mean of X =", round(mean_X, 4), "\n")
    cat("1 / sample_mean =", round(inv_mean, 4), "\n")
    cat("Sample mean of Y =", round(mean_Y, 4), "\n")
    cat("Difference =", round(abs(inv_mean - mean_Y), 4), "\n\n")

    ```

We simulated 100 realizations of X \~ N(10, 27) and computed Y = 1/X, finding that 1/X̄ = 0.0955 while Ȳ = 0.0687, with a difference of 0.0268. This demonstrates that the reciprocal of the mean is not equal to the mean of reciprocals, confirming the nonlinear behavior of the expectation operator. The substantial difference arises because averaging before taking reciprocals (1/X̄) yields a different result than taking reciprocals before averaging (Ȳ), especially when X has variability around zero.

3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

    ```{r}
    set.seed(id)
    X <- rexp(100, rate = 2)
    Y <- rexp(100, rate = 2)

    Z <- log(X) + 5

    qqplot(X, Y,
           main = "Quantile-Quantile plot of X and Y",
           xlab = "X",
           ylab = "Y",
           pch = 19, col = "blue")

    plot(X, Y,
         main = "Scatterplot of X and Y",
         xlab = "X",
         ylab = "Y",
         pch = 19, col = rgb(0, 0, 1, 0.4))

    qqplot(X, Z,
           main = "Quantile-Quantile plot of X and Z",
           xlab = "X",
           ylab = "Y",
           pch = 19, col = "blue")

    plot(X, Z,
         main = "Scatterplot of X and Z",
         xlab = "X",
         ylab = "Y",
         pch = 19, col = rgb(0, 0, 1, 0.4))

    ```

The Quantile-Quantile and scatterplots of X and Y show that both variables have the same exponential distribution but are independent. The points in the scatterplot are spread randomly, meaning X and Y are not related.

The plots of X and Z show a clear curved pattern. This means that Z depends on X. The relation is not random but fully determined.

So, X and Y are similar but independent, while X and Z are dependent.

#### 2. You toss a fair coin three times and a random variable $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost £$1$. They will receive £$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$?

To answer this,

1.  Explain what type of random variable is X:

    -   Normally distributed

    -   Binomially distributed

    -   Poisson distributed

    -   Uniformly distributed

        **Explanation:**

        **Random variable X is Binomially distributed: B(n = 3, p = 0.5), because the experiment was repeated 3 times and each outcome was independent. Probability of favorable is equal to 0.5, because the coin is fair.**

2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    tosses <- 3
    p <- 0.5
    n <- 100

    X <- rbinom(N, size = tosses, prob = p)

    E_x <- tosses * p

    Var_x <- tosses * 0.25

    sample_mean <- mean(X)
    sample_variance <- sum((X - sample_mean)^2)/(n - 1)

    cat("Expectation of binomial distribution of X: ", E_x, "\n")
    cat("Mean of binomial distribution of X on data: ", sample_mean, "\n\n")


    cat("Variance of binomial distribution of X: ", Var_x, "\n")
    cat("Sample variance of binomial distribution of X on data: ", sample_variance, "\n")

    ```

**Here we can see that theoretically provided values of expectation and variance are very close to the values on real data, hence the theory holds true.**

3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    tosses <- 3
    p <- 0.5
    n <- 100

    X <- rbinom(N, size = tosses, prob = p)

    Y <- 0.5 * X - 1

    E_y <- 0.5 * E_x - 1

    Var_y <- 0.25 * Var_x


    sample_mean_y <- mean(Y)
    sample_variance_y <- sum((Y - sample_mean_y)^2)/(n - 1)

    cat("Expectation of binomial distribution of Y: ", E_y, "\n")
    cat("Mean of binomial distribution of Y on data: ", sample_mean_y, "\n\n")


    cat("Variance of binomial distribution of Y: ", Var_y, "\n")
    cat("Sample variance of binomial distribution of Y on data: ", sample_variance_y, "\n")

    ```

**In this part, we can also see that theoretically provided values of expectation and variance are very close to the values on real data, hence the theory holds true.**

------------------------------------------------------------------------

### General summary and conclusions

In Task 1, we analyzed the reliability of the [7,4] Hamming code using simulation. By encoding random binary messages, introducing random bit-flip errors with probability 0.1​, and decoding them with single-error correction, we estimated the probability of successful transmission​. Using the k-sigma rule, we built an interval for the true success probability and determined the number of trials required to keep the estimation error in a given bound.

In Task 2, we demonstrated how the Poisson distribution effectively models radioactive decay processes, with the sample means converging to a normal distribution as the sample size increases, validating the Central Limit Theorem. We confirmed that the empirical cumulative distribution functions closely match their theoretical normal counterparts, with maximal differences decreasing from 0.045 to 0.015 as sample sizes grew from 5 to 50. Normal approximations become increasingly reliable for larger sample sizes in real-world radioactive decay scenarios.

In task 3, in general we observed the usage of the Central Limit Theorem, showed how precisely it works on data, not only in theory. We showed that with increasing amount of samples data gets close to the Standard normal distribution, obtained that Central Limit Theorem provides a great base when we need to find the accurate values of data which is bounded.

In task 4, we worked with topics learned previously, such as data distribution and its properties, explored that the theoretical values provide almost precise result to the ones obtained on generated samples.

While working on this laboratory work, our team explored the previously learned topic on real examples and learned the specifics of working with it.

### Work breakdown

Arsen Botsko: Task 1, Task 4 subtask 1 part 3, Task 4 subtask 2 part 1

Elizabeth Buhir: Task 2, Task 4 subtask 1 part 1, Task 4 subtask 1 part 2

Oksana Moskviak: Task 3, Task 4 subtask 2 part 2, Task 4 subtask 2 part 3
